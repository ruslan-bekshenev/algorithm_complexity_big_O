# Как посчитать сложность алгоритма по BIG O

<b>Ролик с канала <a href="https://www.youtube.com/channel/UCmI5YBB9KJ0xLtFtgBX8rfw">Front-end Science с Сергеем пузанковым</a></b>

ссылка на ролик: https://youtu.be/Fu4BzQNN0Qs

## Что такое Big O

Для того, чтобы сравнивать два алгоритма по сложности чаще всего используется **Big O** (Big O notation), бывают еще:

- Big-Theta
- Big-Omega
- small-o
- small-omega,

но они используеются редко.

Нотация **Big O** это относительное представление сложности алгоритма. Она показывает как будет меняться производительность алгоритма, в зависимости от роста входящих данных.

Если мы будем увеличивать кол-во входящих данных, то могут расти 2 вещи:

- Время за которое будет отрабатывать алгорит
- Кол-во памяти используемое данным алгоритмом для обработки всего объема данных.

Скорость роста этих параметров показывает нам Big O.

## O(1)

```
  // возвращает последний элемент
  function getLastElement(arr) {
    return arr[arr.length-1];
  }
```

Такая функция работает за константное время, вне зависимости от количества данных, которые к нам поступают.

### Константная сложность

| data   | Operations |
| ------ | ---------- |
| 10     | 1          |
| 10 000 | 1          |

## O(n)

```
  function getSumOfArray(arr) {
    let sum = 0;

    for (let i = 0; i < arr.length: i++) {
      sum += arr[i]
    }

    return sum;
  }
```

В данном варианте находится цикл, который перебирает каждый элемент массива и суммирует их с переменной sum. В данном варианте получается линейная зависимость времени необходимого на отработку данного алгоритма от количества данных, которые мы подаем на вход.

| data   | Operations |
| ------ | ---------- |
| 10     | 10         |
| 10 000 | 10 000     |

## O(log n)

```
  // алгоритм бинарного поиска
  let search = function (nums, target) {
    let left = 0;
    let right = nums.length - 1;
    let mid;

    while (left <= right) {
      mid = Math.round((right-left) / 2) + left;

      if (target === nums[mid]) {
        return mid;
      } else if (target < nums[mid]>) {
        right = mid - 1
      } else {
        left = mid + 1;
      }
    }
    return -1;
  }
```

В данном варианте, мы на каждой итерации берем массив входных данных и делим на 2 части, одну из них откидываем. На каждой итерации уменьшаем объем данных, необходимых для обработки вдвое.

| data   | Operations |
| ------ | ---------- |
| 10     | 7          |
| 10 000 | 14         |

## O(n log n)

Сложность алгоритма **merge sort**, которое исполуется в firefox и в chrome. 

## O(n^2)

```
  function getMultiplyList(n) {
    for (let i = 1; i <= n; i++) {
      for (let j = 1; j <= n; j++) {
        console.log(`${i} * ${j} = ${i*j}`);
      }
    }
  }
```

На каждой итерации нашего первого цикла будем делать дополнительно n операций вторым циклом. В результате сложность будет O(n^2)

| data   | Operations  |
| ------ | ----------- |
| 10     | 100         |
| 10 000 | 100 000 000 |

## O(n^3)

Если будет 3 вложенных цикла, то сложность будет кубическая, и она возрастает еще быстрее.

| data   | Operations        |
| ------ | ----------------- |
| 10     | 1 000             |
| 10 000 | 1 000 000 000 000 |

## O(2^n)

Эскпоненциальная сложность

```
  // получение чисел Фибоначчи
  function getFib(n) {
    if (n < 2) {
      return n;
    }

    return getFib(n - 1) + getFib(n - 2);
  }
```

| data   | Operations        |
| ------ | ----------------- |
| 10     | 1 024             |
| 20     | 1 048 576         |

## O(n!)

Есть очень популярная **задача о коммивояжере** где нужно проложить кратчайший пусть между n городами, таким образом, чтобы коммивояжер посетил хотя бы по разу каждый город и вернулся в свой изначальный город из которого он вышел. Для решения задачи придется перебрать все комбинации городов и решение задачи в общем виде будет иметь сложность O(n!)

3! = 3 x 2 x 1 = 6
5! = 5 x 4 x 3 x 2 x 1 = 120
6! = 6 x 5 x 4 x 3 x 2 x 1 = 720
7! = 7 x 6 x 5 x 4 x 3 x 2 x 1 = 5040
25! = 25 x 24 x ... x 2 x 1 = 15,511,210,043,330,985,984,000,000
...

## Big O Complexity chart

[logo]https://github.com/ruslan-bekshenev/algorithm_complexity_big_O/blob/master/images/graph.jpeg)

## Отбрасывание констант и несущественной части

Важным моментом является то, что Big O указывает на самую быстрорастущую сложность и отбразывает константные оптимизации

### Примеры:

O(2n) -> O(n)

O(n + 2^n) -> O(n^2), где "n + " можно отбросить, так как она растет в разы медленнее, чем n^2 

O(n + log n) - можем отбросить log n, так как она растет в разы медленнее, тогда сложность будет O(n)

O(60*2^n + 10*n^100) -> O(2^n + n^100) -> O(2^n)
O(n^2 + m) -> O(n^2 + m) - остается также, потому что m неизвестно

## Практика

```
  function count(n) {
    for (let i = 0; i < n; i++) {
      for (let j = 0; j < n; j++) {
        for (let k = 0; k < 100000; k++) {
          console.log(i + j + k);
        } 
      } 
    } 
  }
```

Ответом будет O(n^2). Почему? Потому что даже при 100000 операций внутри, 100000 - это константа, то есть результирующая сложность была бы O(100000*n^2) -> O(n^2)

```
  function iterate(n) {
    for (let i = 0; i < n: i++) {
      for (let j = 0; j < n: j++) {
        console.log(i + j)
      }
    }
  }
```

Ответ: O(n^2 / 2) => O(n^2)

## Сложность по памяти

Память тоже важна. Будут случаться ситуации, когда будут ограничены ресурсы.

Оценивается также по Big o. Копирование массива

```
  let arr2 = [...arr]
```

Будет O(n), так как с ростом входящих данных, будет пропорционально расти и количество используемой памяти для копии массива. При расчете сложности по памяти учитывается только кол-во дополнительно используемой памяти вашим алгоритмом. Память,, которая аллоцирована для хранения входящих данных в расчет не берется.

```
  // возвращает последний элемент
  function getLastElement(arr) {
    return arr[arr.length-1];
  }
```

Сложность по памяти будет будет тоже O(1).

## Когда оптимизировать производительность?

Например, если на проекте есть массив на 100 элементов и нужно найти число, то не стоит делать бинарный поиск, можно использовать нативный indexOf. Будет в разы проще в написании кода, так и в поддержке.

Но если уже со старта понятно, что будут большие объемы данных, то стоит учесть.